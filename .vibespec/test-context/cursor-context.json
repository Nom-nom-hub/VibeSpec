{
  "project": {
    "name": "Data Pipeline",
    "version": "1.0.0",
    "description": "A robust data processing pipeline that extracts, transforms, and loads data from multiple sources while ensuring data quality, reliability, and performance.\n"
  },
  "goals": [
    "Provide reliable and scalable data processing operations",
    "Ensure data quality and integrity throughout the pipeline",
    "Deliver real-time or near-real-time data processing capabilities",
    "Enable monitoring and observability of pipeline performance",
    "Support multiple data formats and transformation operations"
  ],
  "constraints": [
    "Must handle various data formats (JSON, CSV, XML, Parquet, etc.)",
    "Must support high-volume data processing requirements",
    "Must implement proper error handling and data recovery",
    "Must comply with data privacy and security regulations",
    "Must provide configurable processing and retry logic"
  ],
  "features": [
    {
      "name": "Data Ingestion",
      "description": "Efficient collection and intake of data from various sources",
      "requirements": [
        "Support streaming and batch data ingestion methods",
        "Implement data source connectors for databases and APIs",
        "Handle structured, semi-structured, and unstructured data",
        "Provide data validation and quality checks at intake",
        "Support incremental and full data loading strategies"
      ],
      "flows": []
    },
    {
      "name": "Data Transformation",
      "description": "Comprehensive data processing and transformation operations",
      "requirements": [
        "Enable complex data transformations and aggregations",
        "Support data cleansing and standardization operations",
        "Implement schema validation and data type conversion",
        "Provide conditional logic and business rule processing",
        "Support data deduplication and merge operations"
      ],
      "flows": []
    },
    {
      "name": "Data Quality & Monitoring",
      "description": "Continuous monitoring and quality assurance",
      "requirements": [
        "Implement comprehensive data profiling and validation",
        "Provide real-time pipeline health monitoring",
        "Enable data lineage and dependency tracking",
        "Support automated data quality alerting systems",
        "Create detailed audit logs and error reporting"
      ],
      "flows": []
    }
  ],
  "implementation": {
    "cwd": "C:\\Users\\Kaiden\\Desktop\\VibeSpec",
    "recentFiles": []
  },
  "aiInstructions": {
    "guidelines": [
      "Follow the specification exactly as described",
      "Ask for clarification if requirements are unclear",
      "Focus on implementing one feature at a time",
      "Write clean, well-documented code",
      "Follow established project patterns and conventions"
    ],
    "doNotDo": {
      "do": [
        "Implement features as specified",
        "Ask questions when uncertain",
        "Write tests for new functionality",
        "Document complex logic",
        "Follow security best practices"
      ],
      "dont": [
        "Skip requirements or implementation steps",
        "Ignore error handling",
        "Write overly complex solutions",
        "Deviate from the specification without discussion"
      ]
    },
    "patterns": [
      "Use modular, reusable components",
      "Follow established naming conventions",
      "Implement proper error handling",
      "Write self-documenting code"
    ]
  }
}
